
env = qe
runtime_env = docker

enable_debug = true
enable_dev = false

airflow_home = /home/airflow

namespaces_dir = ${airflow_home}/namespaces

git = {

  name = data
  url = "https://github.com/hamshif/data"
  dir = ${airflow_home}/data
  branch = feature/configure-data
  commit = commit_hash_code

  submodules = {

    provision = {

      name = provision
      url = "https://github.com/hamshif/provision.git"
      dir = ${git.dir}/provision
      branch = master
    }

    dags = {

      name = dags
      url = "https://github.com/hamshif/dags"
      dir = ${git.dir}/
      branch = feature/configure-refine1
    }
  }
}


# use these variables to override airflow.cfg configuration
airflow = {

  dags_folder = ${git.dir}/dags
  base_log_folder = ${airflow_home}/logs

  # Logging level
  logging_level = INFO
  fab_logging_level = WARN

  executor = CeleryExecutor

  # sql_alchemy_conn = sqlite:////home/airflow/airflow.db
  sql_alchemy_conn = "mysql://airflow:1111airflow@localhost:3306/airflow"

  broker_url = "pyamqp://guest:guest@localhost:5672/"

  celery_result_backend = "db+mysql://airflow:1111airflow@localhost:3306/airflow"
}


cloud = {

  gcp = {

    project = rtp-gcp-poc
    region = global
    zone = us-west1-a
    image_repo_zone = lalaland

    service_accounts = {
      dataproc = gid-159"@"rtp-gcp-poc.iam.gserviceaccount.com
    }


    network = default

    services = {

    }

    artifactory_bucket = "gs://gid-ram"
  }
}


spark_jobs = {

//  TODO csv refers to a case of multiple jars this needs a poc

  walk_me_jars_csv = ${cloud.gcp.artifactory_bucket}/Snippets-1.0.0-SNAPSHOT.jar

  walk_me = {
    jars_csv = ${spark_jobs.walk_me_jars_csv}
    main = com.hamshif.wielder.pipelines.snippets.WalkMe
    args = ""
  }
}


template_ignore_dirs = [
  .git
  .terraform
  image
  terra
  db_scripts
  bash-scripts
  __pycache__
  debug-mount
  artifacts
  handy-scripts
  darwin_amd64
  plugins
  bucket
]
